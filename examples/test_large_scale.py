#!/usr/bin/env python3
"""
BioLitKG å¤§è§„æ¨¡åˆ†ææµ‹è¯•è„šæœ¬
ç›´æ¥è¿è¡Œ,æ— éœ€äº¤äº’å¼è¾“å…¥
"""

import os
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from literature import UnifiedLiteratureSearch
from workflow import WorkflowAnalyzer
from workflow.visualizer import WorkflowVisualizer
from utils import CitationEnricher


def print_banner(text):
    """æ‰“å°æ¨ªå¹…"""
    print("\n" + "=" * 80)
    print(text)
    print("=" * 80 + "\n")


def main():
    print_banner("ğŸš€ BioLitKG å¤§è§„æ¨¡åˆ†ææµ‹è¯•")
    
    # ========== é…ç½®å‚æ•° ==========
    
    # æœç´¢é…ç½®
    TOPIC = "single cell RNA sequencing"  # æ ¸å¿ƒä¸»é¢˜(ç®€æ´!)
    MAX_SEARCH = 500     # æœç´¢æ•°é‡(å¹¿æ’’ç½‘)
    MAX_ANALYZE = 150    # åˆ†ææ•°é‡(æ·±åº¦åˆ†æ)
    YEAR_FROM = 2015     # èµ·å§‹å¹´ä»½
    YEAR_TO = 2024       # ç»“æŸå¹´ä»½
    
    # åŠŸèƒ½å¼€å…³
    ENRICH_CITATIONS = True   # è¡¥å……å¼•ç”¨æ•°(æ¨èå¼€å¯)
    USE_ARXIV = True          # ä½¿ç”¨arXiv
    USE_PUBMED = True         # ä½¿ç”¨PubMed
    USE_LLM = True            # AIåˆ†æ
    CREATE_VIZ = True         # ç”Ÿæˆå¯è§†åŒ–
    
    # è¾“å‡ºé…ç½®
    OUTPUT_NAME = f"scrna_large_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    OUTPUT_DIR = Path(f"outputs/{OUTPUT_NAME}")
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # æ˜¾ç¤ºé…ç½®
    print("ğŸ“‹ åˆ†æé…ç½®:")
    print(f"  ä¸»é¢˜: {TOPIC}")
    print(f"  æœç´¢æ•°é‡: {MAX_SEARCH} ç¯‡")
    print(f"  åˆ†ææ•°é‡: {MAX_ANALYZE} ç¯‡")
    print(f"  è¡¥å……å¼•ç”¨æ•°: {'æ˜¯' if ENRICH_CITATIONS else 'å¦'}")
    print(f"  å¹´ä»½èŒƒå›´: {YEAR_FROM}-{YEAR_TO}")
    print(f"  æ•°æ®æº: {'arXiv' if USE_ARXIV else ''}{', ' if USE_ARXIV and USE_PUBMED else ''}{'PubMed' if USE_PUBMED else ''}")
    print(f"  AIåˆ†æ: {'å¯ç”¨' if USE_LLM else 'ç¦ç”¨'}")
    print(f"  å¯è§†åŒ–: {'ç”Ÿæˆ' if CREATE_VIZ else 'ä¸ç”Ÿæˆ'}")
    print(f"  è¾“å‡ºä½ç½®: {OUTPUT_DIR}")
    print()
    
    # æ£€æŸ¥APIå¯†é’¥
    if USE_LLM and not os.getenv('LLM_API_KEY'):
        print("âŒ é”™è¯¯: æœªè®¾ç½® LLM_API_KEY ç¯å¢ƒå˜é‡")
        print("   è¯·å…ˆè¿è¡Œ: export LLM_API_KEY='your-key'")
        sys.exit(1)
    
    try:
        # ========== æ­¥éª¤1: æœç´¢æ–‡çŒ® ==========
        print_banner("æ­¥éª¤1/5: æœç´¢æ–‡çŒ®")
        
        print(f"ğŸ” æœç´¢å…³é”®è¯: {TOPIC}")
        print(f"ğŸ“š ç›®æ ‡æ•°é‡: {MAX_SEARCH} ç¯‡")
        print(f"â±ï¸  é¢„è®¡æ—¶é—´: 1-2åˆ†é’Ÿ")
        print()
        
        # é…ç½®æ•°æ®æº
        sources = []
        if USE_ARXIV:
            sources.append("arxiv")
        if USE_PUBMED:
            sources.append("pubmed")
        
        if not sources:
            print("âŒ é”™è¯¯: è‡³å°‘éœ€è¦é€‰æ‹©ä¸€ä¸ªæ•°æ®æº")
            sys.exit(1)
        
        search = UnifiedLiteratureSearch()
        
        # æ‰§è¡Œæœç´¢
        # max_results_per_source: æ¯ä¸ªæ•°æ®æºè¿”å›çš„ç»“æœæ•°
        # æœ‰2ä¸ªæ•°æ®æº(arXiv+PubMed),æ‰€ä»¥è®¾ç½®ä¸ºMAX_SEARCH//2è®©æ€»æ•°æ¥è¿‘ç›®æ ‡
        papers = search.search(
            query=TOPIC,
            max_results_per_source=MAX_SEARCH // 2,  # æ¯ä¸ªæº250ç¯‡
            total_max_results=MAX_SEARCH  # æ€»å…±æœ€å¤š500ç¯‡
        )
        
        print(f"âœ“ æœç´¢å®Œæˆ: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
        
        if not papers:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡,è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è°ƒæ•´æœç´¢å…³é”®è¯")
            sys.exit(1)
        
        # ========== æ­¥éª¤2: æŒ‰å¹´ä»½ç­›é€‰ ==========
        print_banner("æ­¥éª¤2/5: ç­›é€‰è®ºæ–‡")
        
        print(f"ğŸ“… å¹´ä»½ç­›é€‰: {YEAR_FROM}-{YEAR_TO}")
        papers = [p for p in papers if p.year and YEAR_FROM <= p.year <= YEAR_TO]
        print(f"âœ“ å¹´ä»½ç­›é€‰å: {len(papers)} ç¯‡")
        
        if not papers:
            print(f"âŒ ç­›é€‰åæ²¡æœ‰è®ºæ–‡,è¯·è°ƒæ•´å¹´ä»½èŒƒå›´")
            sys.exit(1)
        
        # ========== æ­¥éª¤3: è¡¥å……å¼•ç”¨æ•° ==========
        if ENRICH_CITATIONS:
            print_banner("æ­¥éª¤3/5: è¡¥å……å¼•ç”¨æ•°")
            
            print(f"ğŸ“Š ä½¿ç”¨ Semantic Scholar è¡¥å……å¼•ç”¨æ•°")
            print(f"ğŸ“š éœ€è¦å¤„ç†: {len(papers)} ç¯‡")
            print(f"â±ï¸  é¢„è®¡æ—¶é—´: {len(papers) // 50 + 1}-{len(papers) // 30 + 2} åˆ†é’Ÿ")
            print()
            
            try:
                enricher = CitationEnricher()
                papers = enricher.enrich_citations(papers, show_progress=True)
                
                # ç»Ÿè®¡
                with_citations = sum(1 for p in papers if p.citation_count and p.citation_count > 0)
                print(f"\nâœ“ å¼•ç”¨æ•°è¡¥å……å®Œæˆ: {with_citations}/{len(papers)} ç¯‡æœ‰å¼•ç”¨æ•°æ®")
                
                # æŒ‰å¼•ç”¨æ•°æ’åº
                papers = sorted(papers, key=lambda p: p.citation_count or 0, reverse=True)
                print(f"âœ“ å·²æŒ‰å¼•ç”¨æ•°é™åºæ’åº")
                
            except ImportError:
                print("âš ï¸  semanticscholar åŒ…æœªå®‰è£…,è·³è¿‡å¼•ç”¨æ•°è¡¥å……")
                print("   å®‰è£…: pip install semanticscholar")
                # æŒ‰å¹´ä»½æ’åº
                papers = sorted(papers, key=lambda p: p.year or 0, reverse=True)
            except Exception as e:
                print(f"âš ï¸  å¼•ç”¨æ•°è¡¥å……å¤±è´¥: {e}")
                papers = sorted(papers, key=lambda p: p.year or 0, reverse=True)
        else:
            print_banner("æ­¥éª¤3/5: è·³è¿‡å¼•ç”¨æ•°è¡¥å……")
            # æŒ‰å¹´ä»½æ’åº
            papers = sorted(papers, key=lambda p: p.year or 0, reverse=True)
            print(f"âœ“ å·²æŒ‰å¹´ä»½é™åºæ’åº")
        
        # é™åˆ¶æ•°é‡
        papers = papers[:MAX_ANALYZE]
        print(f"\nâœ“ æœ€ç»ˆé€‰æ‹©: {len(papers)} ç¯‡è®ºæ–‡è¿›è¡Œåˆ†æ")
        
        # æ˜¾ç¤ºTop 5
        print(f"\nğŸ“Š Top 5 è®ºæ–‡:")
        for i, p in enumerate(papers[:5], 1):
            cit = p.citation_count or 0
            year = p.year or '?'
            print(f"  {i}. [{cit:4d}å¼•, {year}å¹´] {p.title[:65]}...")
        
        # ========== æ­¥éª¤4: åˆ†æå·¥ä½œæµç¨‹ ==========
        print_banner("æ­¥éª¤4/5: åˆ†æå·¥ä½œæµç¨‹")
        
        print(f"ğŸ¤– AIåˆ†æ: {'å¯ç”¨' if USE_LLM else 'ç¦ç”¨'}")
        print(f"ğŸ“š åˆ†æè®ºæ–‡: {len(papers)} ç¯‡")
        print(f"â±ï¸  é¢„è®¡æ—¶é—´: {2 + len(papers) // 50} åˆ†é’Ÿ")
        print()
        
        analyzer = WorkflowAnalyzer(
            use_llm=USE_LLM,
            max_papers=MAX_ANALYZE,
            year_from=YEAR_FROM,
            year_to=YEAR_TO
        )
        
        result = analyzer.analyze_workflow(papers)
        
        print(f"âœ“ è¯†åˆ«æ­¥éª¤: {len(result['steps'])} ä¸ª")
        print(f"âœ“ è¯†åˆ«å·¥å…·: {len(result['tools'])} ä¸ª")
        
        # ========== æ­¥éª¤5: ç”ŸæˆæŠ¥å‘Šå’Œå¯è§†åŒ– ==========
        print_banner("æ­¥éª¤5/5: ç”ŸæˆæŠ¥å‘Šå’Œå¯è§†åŒ–")
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = OUTPUT_DIR / "WORKFLOW_REPORT.md"
        analyzer.generate_workflow_report(result, output_path=report_file)
        print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # ç”Ÿæˆè®ºæ–‡æ¸…å• (ç®€åŒ–ç‰ˆ)
        papers_list_file = OUTPUT_DIR / "papers_list.md"
        papers_list_content = "# è®ºæ–‡æ¸…å•\n\n"
        papers_list_content += f"å…± {len(papers)} ç¯‡è®ºæ–‡\n\n"
        for i, paper in enumerate(papers, 1):
            # å¤„ç†ä½œè€…åç§° (æ”¯æŒå­—ç¬¦ä¸²æˆ–Authorå¯¹è±¡)
            if paper.authors:
                author_names = []
                for a in paper.authors[:3]:
                    if isinstance(a, str):
                        author_names.append(a)
                    else:
                        # Authorå¯¹è±¡,å°è¯•å¤šä¸ªå±æ€§
                        name = getattr(a, 'name', None) or getattr(a, 'full_name', None) or str(a)
                        author_names.append(name)
                authors_str = ', '.join(author_names)
                if len(paper.authors) > 3:
                    authors_str += ' ç­‰'
            else:
                authors_str = "æœªçŸ¥"
            
            papers_list_content += f"{i}. **{paper.title}**\n"
            papers_list_content += f"   - ä½œè€…: {authors_str}\n"
            papers_list_content += f"   - å¹´ä»½: {paper.year}\n"
            papers_list_content += f"   - å¼•ç”¨æ•°: {paper.citation_count or 0}\n"
            if paper.doi:
                papers_list_content += f"   - DOI: [{paper.doi}](https://doi.org/{paper.doi})\n"
            if paper.arxiv_id:
                papers_list_content += f"   - arXiv: [{paper.arxiv_id}](https://arxiv.org/abs/{paper.arxiv_id})\n"
            if paper.pubmed_id:
                papers_list_content += f"   - PubMed: [PMID:{paper.pubmed_id}](https://pubmed.ncbi.nlm.nih.gov/{paper.pubmed_id}/)\n"
            papers_list_content += "\n"
        
        papers_list_file.write_text(papers_list_content, encoding='utf-8')
        print(f"âœ“ è®ºæ–‡æ¸…å•å·²ä¿å­˜: {papers_list_file}")
        
        # ç”Ÿæˆå¯è§†åŒ–
        if CREATE_VIZ:
            print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")
            
            visualizer = WorkflowVisualizer()
            
            # çŸ¥è¯†å›¾è°±ç½‘ç»œ
            network_file = OUTPUT_DIR / "workflow_network.html"
            visualizer.create_workflow_network(
                workflow_result=result,
                papers=papers,
                output_path=str(network_file)
            )
            print(f"âœ“ ç½‘ç»œå›¾å·²ä¿å­˜: {network_file}")
            
            # ç»Ÿè®¡å›¾è¡¨
            stats_file = OUTPUT_DIR / "paper_statistics.html"
            visualizer.create_paper_statistics(
                papers=papers,
                output_path=str(stats_file)
            )
            print(f"âœ“ ç»Ÿè®¡å›¾å·²ä¿å­˜: {stats_file}")
        
        # ========== å®Œæˆ ==========
        print_banner("âœ… åˆ†æå®Œæˆ!")
        
        print("ğŸ“‚ è¾“å‡ºæ–‡ä»¶:")
        print(f"  ğŸ“„ {report_file}")
        print(f"  ğŸ“„ {papers_list_file}")
        if CREATE_VIZ:
            print(f"  ğŸŒ {OUTPUT_DIR / 'workflow_network.html'}")
            print(f"  ğŸ“Š {OUTPUT_DIR / 'paper_statistics.html'}")
        
        print(f"\nğŸ’¡ æŸ¥çœ‹å¯è§†åŒ–:")
        print(f"  open {OUTPUT_DIR / 'workflow_network.html'}")
        print(f"  open {OUTPUT_DIR / 'paper_statistics.html'}")
        
        print(f"\nğŸ“Š åˆ†æç»Ÿè®¡:")
        print(f"  æœç´¢è®ºæ–‡: {MAX_SEARCH} ç¯‡")
        print(f"  åˆ†æè®ºæ–‡: {len(papers)} ç¯‡")
        print(f"  è¯†åˆ«æ­¥éª¤: {len(result['steps'])} ä¸ª")
        print(f"  è¯†åˆ«å·¥å…·: {len(result['tools'])} ä¸ª")
        
        if ENRICH_CITATIONS:
            with_cit = sum(1 for p in papers if p.citation_count and p.citation_count > 0)
            avg_cit = sum(p.citation_count or 0 for p in papers) / len(papers)
            print(f"  å¼•ç”¨æ•°æ®: {with_cit}/{len(papers)} ç¯‡")
            print(f"  å¹³å‡å¼•ç”¨: {avg_cit:.1f} æ¬¡")
        
        print("\nğŸ‰ æˆåŠŸ!")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

