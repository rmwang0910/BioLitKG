#!/usr/bin/env python
"""
BioLitKG äº¤äº’å¼åˆ†æ

é€šè¿‡å¯¹è¯æ–¹å¼æŒ‡å®šåˆ†æå†…å®¹å’Œå‚æ•°
"""
import os
import sys
import logging
from pathlib import Path
from datetime import datetime

# é…ç½®logging
logging.basicConfig(
    level=logging.WARNING,  # å‡å°‘è¾“å‡º
    format='%(message)s'
)

# é…ç½®LLM (ä»ç¯å¢ƒå˜é‡è¯»å–)
if not os.getenv('LLM_API_KEY'):
    print("âš ï¸  è¯·å…ˆè®¾ç½®APIå¯†é’¥: export LLM_API_KEY='your-key'")
    print("   æˆ–åœ¨è¿è¡Œæ—¶è¾“å…¥")

from workflow import WorkflowAnalyzer, WorkflowVisualizer
from literature import UnifiedLiteratureSearch


def get_input(prompt, default=None, input_type=str):
    """è·å–ç”¨æˆ·è¾“å…¥"""
    if default:
        prompt = f"{prompt} [é»˜è®¤: {default}]"
    
    while True:
        try:
            value = input(f"{prompt}: ").strip()
            if not value and default is not None:
                return default
            if not value:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆå€¼")
                continue
            
            if input_type == int:
                return int(value)
            elif input_type == bool:
                return value.lower() in ['y', 'yes', 'æ˜¯', 't', 'true']
            else:
                return value
        except ValueError:
            print(f"âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„{input_type.__name__}ç±»å‹")
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å·²å–æ¶ˆ")
            sys.exit(0)


def interactive_analysis():
    """äº¤äº’å¼åˆ†ææµç¨‹"""
    
    print("=" * 80)
    print("ğŸ§¬ BioLitKG äº¤äº’å¼æ–‡çŒ®åˆ†æ")
    print("=" * 80)
    print()
    print("é€šè¿‡å¯¹è¯æ–¹å¼è®¾ç½®åˆ†æå‚æ•°")
    print("æç¤º: ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼,Ctrl+Cå–æ¶ˆ")
    print()
    
    # ===== 1. åˆ†æä¸»é¢˜ =====
    print("ğŸ“‹ æ­¥éª¤1: åˆ†æä¸»é¢˜")
    print("-" * 80)
    
    topic = get_input("è¯·è¾“å…¥åˆ†æä¸»é¢˜", "single cell RNA sequencing")
    
    # æ¨èå…³é”®è¯
    print("\nğŸ’¡ æ¨èæ·»åŠ çš„å…³é”®è¯(å¯é€‰):")
    print("  1. review - ä¼˜å…ˆç»¼è¿°")
    print("  2. benchmark - ä¼˜å…ˆæ¯”è¾ƒç ”ç©¶")
    print("  3. protocol - ä¼˜å…ˆå®éªŒæ–¹æ¡ˆ")
    print("  4. best practice - ä¼˜å…ˆæœ€ä½³å®è·µ")
    
    add_keywords = get_input("\næ˜¯å¦æ·»åŠ å…³é”®è¯? (y/n)", "y", bool)
    
    if add_keywords:
        print("\né€‰æ‹©æ·»åŠ çš„å…³é”®è¯:")
        keywords = []
        if get_input("  æ·»åŠ  'review'? (y/n)", "y", bool):
            keywords.append("review")
        if get_input("  æ·»åŠ  'benchmark'? (y/n)", "n", bool):
            keywords.append("benchmark")
        if get_input("  æ·»åŠ  'protocol'? (y/n)", "n", bool):
            keywords.append("protocol")
        if get_input("  æ·»åŠ  'best practice'? (y/n)", "n", bool):
            keywords.append("best practice")
        
        if keywords:
            topic = f"{topic} {' '.join(keywords)}"
    
    print(f"\nâœ“ æœ€ç»ˆæœç´¢è¯: {topic}")
    
    # ===== 2. æ–‡çŒ®æ•°é‡ =====
    print(f"\nğŸ“Š æ­¥éª¤2: æ–‡çŒ®æ•°é‡")
    print("-" * 80)
    
    max_search = get_input("æœ€å¤šæœç´¢å¤šå°‘ç¯‡è®ºæ–‡?", 100, int)
    max_analyze = get_input("æœ€å¤šåˆ†æå¤šå°‘ç¯‡è®ºæ–‡?", 50, int)
    
    # ===== 3. å½±å“åŠ›ç­›é€‰ =====
    print(f"\nâ­ æ­¥éª¤3: å½±å“åŠ›ç­›é€‰(å¼•ç”¨æ•°)")
    print("-" * 80)
    print("ğŸ’¡ å¼•ç”¨æ•°å‚è€ƒ:")
    print("  >500: æé«˜å½±å“åŠ›(Nature/Cell/Scienceçº§åˆ«)")
    print("  >200: é«˜å½±å“åŠ›")
    print("  >100: ä¸­é«˜å½±å“åŠ›")
    print("  >50:  ä¸­ç­‰å½±å“åŠ›")
    print("  >20:  ä¸€èˆ¬å½±å“åŠ›")
    print("  0:    ä¸ç­›é€‰(åŒ…å«æ–°è®ºæ–‡)")
    
    min_citations = get_input("\næœ€å°‘å¼•ç”¨æ•°", 20, int)
    
    # ===== 4. å¹´ä»½èŒƒå›´ =====
    print(f"\nğŸ“… æ­¥éª¤4: å¹´ä»½èŒƒå›´")
    print("-" * 80)
    
    current_year = datetime.now().year
    year_from = get_input(f"èµ·å§‹å¹´ä»½ (å»ºè®®2015-{current_year})", 2018, int)
    year_to = get_input(f"ç»“æŸå¹´ä»½", current_year, int)
    
    # ===== 5. æ•°æ®æºé€‰æ‹© =====
    print(f"\nğŸ” æ­¥éª¤5: æ•°æ®æº")
    print("-" * 80)
    print("å¯ç”¨æ•°æ®æº:")
    print("  - arXiv: é¢„å°æœ¬(è®¡ç®—æœº/ç‰©ç†/ç”Ÿç‰©)")
    print("  - PubMed: ç”Ÿç‰©åŒ»å­¦ä¸»åº“")
    
    use_arxiv = get_input("\nä½¿ç”¨arXiv? (y/n)", "y", bool)
    use_pubmed = get_input("ä½¿ç”¨PubMed? (y/n)", "y", bool)
    
    # ===== 6. é«˜çº§é€‰é¡¹ =====
    print(f"\nâš™ï¸  æ­¥éª¤6: é«˜çº§é€‰é¡¹")
    print("-" * 80)
    
    use_llm = get_input("å¯ç”¨AIåˆ†æ? (y/n)", "y", bool)
    create_viz = get_input("ç”Ÿæˆå¯è§†åŒ–? (y/n)", "y", bool)
    
    # ===== 7. è¾“å‡ºä½ç½® =====
    print(f"\nğŸ“ æ­¥éª¤7: è¾“å‡ºè®¾ç½®")
    print("-" * 80)
    
    output_name = get_input("è¾“å‡ºç›®å½•åç§°", "my_analysis")
    output_dir = Path(f"outputs/{output_name}")
    
    # ===== ç¡®è®¤é…ç½® =====
    print(f"\n" + "=" * 80)
    print("ğŸ“‹ é…ç½®ç¡®è®¤")
    print("=" * 80)
    print(f"\nåˆ†æä¸»é¢˜: {topic}")
    print(f"æœç´¢æ•°é‡: æœ€å¤š{max_search}ç¯‡")
    print(f"åˆ†ææ•°é‡: æœ€å¤š{max_analyze}ç¯‡")
    print(f"å¼•ç”¨ç­›é€‰: >={min_citations}æ¬¡")
    print(f"å¹´ä»½èŒƒå›´: {year_from}-{year_to}")
    print(f"æ•°æ®æº: arXiv={'âœ“' if use_arxiv else 'âœ—'}, PubMed={'âœ“' if use_pubmed else 'âœ—'}")
    print(f"AIåˆ†æ: {'å¯ç”¨' if use_llm else 'ç¦ç”¨'}")
    print(f"å¯è§†åŒ–: {'ç”Ÿæˆ' if create_viz else 'ä¸ç”Ÿæˆ'}")
    print(f"è¾“å‡ºä½ç½®: {output_dir}")
    
    confirm = get_input("\nå¼€å§‹åˆ†æ? (y/n)", "y", bool)
    if not confirm:
        print("ğŸ‘‹ å·²å–æ¶ˆ")
        return
    
    # ===== å¼€å§‹åˆ†æ =====
    print(f"\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹åˆ†æ")
    print("=" * 80)
    
    # æ£€æŸ¥APIå¯†é’¥
    if use_llm and not os.getenv('LLM_API_KEY'):
        api_key = get_input("\nè¯·è¾“å…¥æ‚¨çš„LLM APIå¯†é’¥")
        os.environ['LLM_API_KEY'] = api_key
        os.environ['LLM_BASE_URL'] = os.getenv('LLM_BASE_URL', 'https://dashscope.aliyuncs.com/compatible-mode/v1')
        os.environ['LLM_MODEL'] = os.getenv('LLM_MODEL', 'qwen-plus')
    
    # 1. æœç´¢æ–‡çŒ®
    print(f"\næ­¥éª¤1/5: æœç´¢æ–‡çŒ®...")
    print(f"  å…³é”®è¯: {topic}")
    print(f"  æ•°æ®æº: {'arXiv, ' if use_arxiv else ''}{'PubMed' if use_pubmed else ''}")
    
    search = UnifiedLiteratureSearch()
    papers = search.search(query=topic, max_results=max_search)
    
    print(f"âœ“ æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    
    if not papers:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡,è¯·è°ƒæ•´æœç´¢è¯")
        return
    
    # 2. ç­›é€‰è®ºæ–‡
    print(f"\næ­¥éª¤2/5: ç­›é€‰è®ºæ–‡...")
    
    # å¹´ä»½ç­›é€‰
    papers = [p for p in papers if p.year and year_from <= p.year <= year_to]
    print(f"  å¹´ä»½ç­›é€‰({year_from}-{year_to}): {len(papers)} ç¯‡")
    
    # å¼•ç”¨æ•°ç­›é€‰
    if min_citations > 0:
        papers = [p for p in papers if p.citation_count and p.citation_count >= min_citations]
        print(f"  å¼•ç”¨æ•°ç­›é€‰(>={min_citations}): {len(papers)} ç¯‡")
    
    # æ’åºå’Œé™åˆ¶æ•°é‡
    papers = sorted(papers, key=lambda p: p.citation_count or 0, reverse=True)
    papers = papers[:max_analyze]
    print(f"  æœ€ç»ˆé€‰æ‹©: {len(papers)} ç¯‡")
    
    if not papers:
        print("âŒ ç­›é€‰åæ²¡æœ‰è®ºæ–‡,è¯·é™ä½ç­›é€‰æ¡ä»¶")
        return
    
    # æ˜¾ç¤ºTop 5
    print(f"\n  Top 5è®ºæ–‡:")
    for i, p in enumerate(papers[:5], 1):
        cit = p.citation_count or 0
        print(f"    {i}. [{cit}å¼•, {p.year}] {p.title[:60]}...")
    
    # 3. åˆ†æå·¥ä½œæµç¨‹
    print(f"\næ­¥éª¤3/5: åˆ†æå·¥ä½œæµç¨‹...")
    
    analyzer = WorkflowAnalyzer(
        use_llm=use_llm,
        max_papers=max_analyze,
        min_citations=min_citations,
        year_from=year_from,
        year_to=year_to
    )
    
    result = analyzer.analyze_workflow(papers)
    
    print(f"âœ“ è¯†åˆ«äº† {len(result['steps'])} ä¸ªæ­¥éª¤")
    print(f"âœ“ è¯†åˆ«äº† {len(result['tools'])} ä¸ªå·¥å…·")
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    print(f"\næ­¥éª¤4/5: ç”ŸæˆæŠ¥å‘Š...")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # MarkdownæŠ¥å‘Š
    report_path = output_dir / "WORKFLOW_REPORT.md"
    analyzer.generate_workflow_report(result, str(report_path))
    print(f"âœ“ {report_path}")
    
    # è®ºæ–‡æ¸…å•
    papers_path = output_dir / "PAPERS_LIST.md"
    with open(papers_path, 'w', encoding='utf-8') as f:
        f.write(f"# åˆ†æçš„è®ºæ–‡æ¸…å•\n\n")
        f.write(f"**åˆ†æä¸»é¢˜**: {topic}\n")
        f.write(f"**æ€»æ•°**: {len(papers)} ç¯‡\n")
        f.write(f"**ç­›é€‰æ¡ä»¶**: å¹´ä»½{year_from}-{year_to}, å¼•ç”¨æ•°>={min_citations}\n\n")
        
        for i, paper in enumerate(papers, 1):
            f.write(f"## {i}. {paper.title}\n\n")
            
            # ä½œè€…
            if paper.authors:
                author_names = []
                for a in paper.authors[:3]:
                    if isinstance(a, str):
                        author_names.append(a)
                    else:
                        author_names.append(a.name if hasattr(a, 'name') else str(a))
                authors = ', '.join(author_names)
                if len(paper.authors) > 3:
                    authors += ' ç­‰'
                f.write(f"- **ä½œè€…**: {authors}\n")
            
            f.write(f"- **å¹´ä»½**: {paper.year or 'N/A'}\n")
            f.write(f"- **å¼•ç”¨æ•°**: {paper.citation_count or 0}\n")
            
            if paper.doi:
                f.write(f"- **DOI**: [{paper.doi}](https://doi.org/{paper.doi})\n")
            if paper.arxiv_id:
                f.write(f"- **arXiv**: [{paper.arxiv_id}](https://arxiv.org/abs/{paper.arxiv_id})\n")
            if paper.pubmed_id:
                f.write(f"- **PubMed**: [PMID:{paper.pubmed_id}](https://pubmed.ncbi.nlm.nih.gov/{paper.pubmed_id}/)\n")
            
            if paper.abstract:
                f.write(f"- **æ‘˜è¦**: {paper.abstract[:200]}...\n")
            
            f.write(f"\n")
    
    print(f"âœ“ {papers_path}")
    
    # 5. ç”Ÿæˆå¯è§†åŒ–(å¯é€‰)
    if create_viz:
        print(f"\næ­¥éª¤5/5: ç”Ÿæˆå¯è§†åŒ–...")
        
        visualizer = WorkflowVisualizer()
        
        # ç½‘ç»œå›¾
        network_path = output_dir / "workflow_network.html"
        visualizer.create_workflow_network(result, papers, str(network_path))
        print(f"âœ“ ç½‘ç»œå›¾: {network_path}")
        
        # å·¥å…·å¯¹æ¯”
        if result['tools']:
            tool_path = output_dir / "tool_comparison.html"
            visualizer.create_tool_comparison_chart(result, str(tool_path))
            print(f"âœ“ å·¥å…·å¯¹æ¯”: {tool_path}")
        
        # ç»Ÿè®¡å›¾è¡¨
        stats_path = output_dir / "paper_statistics.html"
        visualizer.create_paper_statistics(papers, str(stats_path))
        print(f"âœ“ ç»Ÿè®¡å›¾è¡¨: {stats_path}")
    
    # ===== å®Œæˆ =====
    print(f"\n" + "=" * 80)
    print("ğŸ‰ åˆ†æå®Œæˆ!")
    print("=" * 80)
    
    print(f"\nğŸ“Š åˆ†æç»“æœ:")
    print(f"  è®ºæ–‡æ•°: {len(papers)}")
    print(f"  æ­¥éª¤æ•°: {len(result['steps'])}")
    print(f"  å·¥å…·æ•°: {len(result['tools'])}")
    
    print(f"\nğŸ”¬ è¯†åˆ«çš„æ­¥éª¤:")
    for step in sorted(result['steps'].values(), key=lambda s: s.order or 999):
        print(f"  {step.order+1 if step.order is not None else '?'}. {step.name}")
    
    print(f"\nğŸ› ï¸  å¸¸ç”¨å·¥å…· (Top 5):")
    for i, (tool, tool_papers) in enumerate(list(result['tools'].items())[:5], 1):
        print(f"  {i}. {tool} - {len(tool_papers)}ç¯‡")
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - {report_path}")
    print(f"  - {papers_path}")
    if create_viz:
        print(f"  - {network_path}")
        print(f"  - {tool_path if result['tools'] else 'N/A'}")
        print(f"  - {stats_path}")
    
    print(f"\nğŸ’¡ æŸ¥çœ‹ç»“æœ:")
    print(f"  cat {report_path}")
    if create_viz:
        print(f"  open {network_path}")
    
    print(f"\nâœ¨ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")


def quick_mode():
    """å¿«é€Ÿæ¨¡å¼ - é€šè¿‡å‘½ä»¤è¡Œå‚æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='BioLitKG å¿«é€Ÿåˆ†æ')
    parser.add_argument('topic', help='åˆ†æä¸»é¢˜')
    parser.add_argument('--max-papers', type=int, default=50, help='æœ€å¤šåˆ†æè®ºæ–‡æ•°')
    parser.add_argument('--min-citations', type=int, default=20, help='æœ€å°‘å¼•ç”¨æ•°')
    parser.add_argument('--year-from', type=int, default=2018, help='èµ·å§‹å¹´ä»½')
    parser.add_argument('--year-to', type=int, default=datetime.now().year, help='ç»“æŸå¹´ä»½')
    parser.add_argument('--no-viz', action='store_true', help='ä¸ç”Ÿæˆå¯è§†åŒ–')
    parser.add_argument('--output', default='quick_analysis', help='è¾“å‡ºç›®å½•å')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ å¿«é€Ÿåˆ†æ: {args.topic}")
    print(f"   è®ºæ–‡æ•°: {args.max_papers}, å¼•ç”¨>={args.min_citations}, å¹´ä»½{args.year_from}-{args.year_to}")
    
    # æ‰§è¡Œåˆ†æ (ä½¿ç”¨argsä¸­çš„å‚æ•°)
    # ... (å®ç°ç±»ä¼¼interactive_analysisçš„é€»è¾‘)
    
    print("âœ“ å®Œæˆ!")


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] not in ['-h', '--help']:
        quick_mode()
    else:
        interactive_analysis()

